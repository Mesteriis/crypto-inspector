#!/usr/bin/env python3
"""
Full Backtesting with Enhanced Accuracy System

Run comprehensive backtesting using the enhanced prediction system
across all default cryptocurrencies.
"""

import asyncio
import json
import logging
import sys
from datetime import datetime
from pathlib import Path

import numpy as np

# Add src to path
sys.path.insert(0, str(Path(__file__).parent.parent / "src"))

from enhanced_accuracy_system import EnhancedAccuracyForecaster

from core.constants import DEFAULT_SYMBOLS
from services.candlestick import CandleInterval, fetch_candlesticks

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(name)s: %(message)s",
    handlers=[
        logging.StreamHandler(sys.stdout),
        logging.FileHandler("full_enhanced_backtest.log"),
    ],
)

logger = logging.getLogger(__name__)


class FullEnhancedBacktester:
    """Full backtesting system with enhanced accuracy."""

    def __init__(self):
        self.enhancer = EnhancedAccuracyForecaster()
        # Start with default symbols and add more
        symbols = DEFAULT_SYMBOLS.copy()

        # Add more popular cryptocurrencies for comprehensive testing
        additional_symbols = [
            "SOL/USDT",  # Solana
            "ADA/USDT",  # Cardano
            "DOT/USDT",  # Polkadot
            "AVAX/USDT",  # Avalanche
            "LINK/USDT",  # Chainlink
            "MATIC/USDT",  # Polygon
            "UNI/USDT",  # Uniswap
            "LTC/USDT",  # Litecoin
            "XRP/USDT",  # Ripple
            "DOGE/USDT",  # Dogecoin
        ]

        symbols.extend(additional_symbols)
        self.default_symbols = symbols
        self.test_window = 30  # Days for testing
        self.prediction_horizon = 3  # Days ahead

    async def run_comprehensive_backtest(self):
        """Run full backtesting across all cryptocurrencies."""
        logger.info("=" * 100)
        logger.info("üöÄ –ü–û–õ–ù–´–ô –ë–≠–ö–¢–ï–°–¢–ò–ù–ì –° –£–õ–£–ß–®–ï–ù–ù–û–ô –°–ò–°–¢–ï–ú–û–ô –¢–û–ß–ù–û–°–¢–ò")
        logger.info("=" * 100)

        results_summary = {
            "total_symbols": len(self.default_symbols),
            "successful_analyses": 0,
            "failed_analyses": 0,
            "model_performance": {},
            "ensemble_performance": [],
            "timestamp": datetime.now().isoformat(),
        }

        for i, symbol in enumerate(self.default_symbols, 1):
            logger.info(f"\n[{i}/{len(self.default_symbols)}] üìä –ê–ù–ê–õ–ò–ó–ò–†–£–ï–ú {symbol}")
            logger.info("-" * 80)

            try:
                result = await self.analyze_symbol(symbol)
                if result:
                    results_summary["successful_analyses"] += 1
                    results_summary["ensemble_performance"].append(
                        {
                            "symbol": symbol,
                            "accuracy": result["ensemble_accuracy"],
                            "confidence": result["ensemble_confidence"],
                            "predictions_count": len(result["predictions"]),
                        }
                    )

                    # Aggregate model performance
                    for model_name, perf in result["model_performance"].items():
                        if model_name not in results_summary["model_performance"]:
                            results_summary["model_performance"][model_name] = {
                                "accuracies": [],
                                "confidences": [],
                                "counts": 0,
                            }
                        results_summary["model_performance"][model_name]["accuracies"].append(perf["accuracy"])
                        results_summary["model_performance"][model_name]["confidences"].append(perf["avg_confidence"])
                        results_summary["model_performance"][model_name]["counts"] += perf["count"]

                else:
                    results_summary["failed_analyses"] += 1

            except Exception as e:
                logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {symbol}: {e}")
                results_summary["failed_analyses"] += 1
                continue

        # Generate final report
        await self.generate_final_report(results_summary)

        return results_summary

    async def analyze_symbol(self, symbol: str) -> dict:
        """Analyze single symbol with enhanced system."""
        try:
            # Fetch extensive historical data
            candles = await fetch_candlesticks(
                symbol=symbol,
                interval=CandleInterval.DAY_1,
                limit=500,  # More data for better analysis
            )

            if not candles or len(candles) < 100:
                logger.warning(f"–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {symbol}")
                return None

            prices = [float(candle.close_price) for candle in candles]
            logger.info(f"–ü–æ–ª—É—á–µ–Ω–æ {len(prices)} –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö —Ü–µ–Ω")

            # Sliding window backtesting
            predictions_data = []
            correct_predictions = 0
            total_predictions = 0

            # Test on last 30 days with rolling window
            test_start_idx = len(prices) - self.test_window - self.prediction_horizon

            for i in range(test_start_idx, len(prices) - self.prediction_horizon):
                # Use data up to point i for prediction
                train_prices = prices[: i + 1]
                actual_future_price = prices[i + self.prediction_horizon]
                current_price = prices[i]

                try:
                    # Enhanced prediction
                    results = await self.enhancer.enhanced_predict(
                        symbol=symbol, interval="1d", prices=train_prices, horizon=self.prediction_horizon
                    )

                    if not results:
                        continue

                    # Store predictions for analysis
                    prediction_record = {
                        "timestamp": datetime.now(),
                        "current_price": current_price,
                        "predicted_price": None,
                        "actual_price": actual_future_price,
                        "models_used": list(results.keys()),
                        "model_predictions": {},
                    }

                    # Individual model analysis
                    model_correct = {}
                    for model_name, result in results.items():
                        pred_price = result["predictions"][-1]
                        direction_correct = (pred_price > current_price) == (actual_future_price > current_price)

                        model_correct[model_name] = direction_correct
                        prediction_record["model_predictions"][model_name] = {
                            "predicted_price": pred_price,
                            "confidence": result["confidence"],
                            "direction_correct": direction_correct,
                        }

                    # Ensemble prediction
                    if len(results) > 1:
                        ensemble_result = self.enhancer.create_weighted_ensemble(results, current_price)
                        ensemble_pred = ensemble_result["price"]
                        ensemble_correct = (ensemble_pred > current_price) == (actual_future_price > current_price)

                        prediction_record["predicted_price"] = ensemble_pred
                        prediction_record["ensemble_correct"] = ensemble_correct
                        prediction_record["ensemble_confidence"] = ensemble_result["confidence"]

                        if ensemble_correct:
                            correct_predictions += 1
                        total_predictions += 1

                    predictions_data.append(prediction_record)

                except Exception as e:
                    logger.debug(f"–ü—Ä–æ–ø—É—â–µ–Ω –ø—Ä–æ–≥–Ω–æ–∑ –¥–ª—è {symbol} –Ω–∞ —à–∞–≥–µ {i}: {e}")
                    continue

            if total_predictions == 0:
                logger.warning(f"–ù–µ—Ç —É—Å–ø–µ—à–Ω—ã—Ö –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –¥–ª—è {symbol}")
                return None

            # Calculate performance metrics
            ensemble_accuracy = (correct_predictions / total_predictions) * 100

            model_performance = {}
            for model_name in self.enhancer.available_models:
                correct_count = sum(
                    1
                    for pred in predictions_data
                    if pred.get("model_predictions", {}).get(model_name, {}).get("direction_correct")
                )
                total_count = len(
                    [pred for pred in predictions_data if model_name in pred.get("model_predictions", {})]
                )

                if total_count > 0:
                    accuracy = (correct_count / total_count) * 100
                    avg_confidence = np.mean(
                        [
                            pred["model_predictions"][model_name]["confidence"]
                            for pred in predictions_data
                            if model_name in pred.get("model_predictions", {})
                        ]
                    )

                    model_performance[model_name] = {
                        "accuracy": accuracy,
                        "avg_confidence": avg_confidence,
                        "count": total_count,
                    }

            # Average ensemble confidence
            avg_ensemble_confidence = (
                np.mean([pred["ensemble_confidence"] for pred in predictions_data if "ensemble_confidence" in pred])
                if predictions_data
                else 0
            )

            logger.info(f"üìä –†–ï–ó–£–õ–¨–¢–ê–¢–´ {symbol}:")
            logger.info(f"   –ê–Ω—Å–∞–º–±–ª—å: {ensemble_accuracy:.1f}% —Ç–æ—á–Ω–æ—Å—Ç–∏ ({total_predictions} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤)")
            logger.info(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_ensemble_confidence:.1f}%")

            for model_name, perf in model_performance.items():
                logger.info(
                    f"   {model_name:15}: {perf['accuracy']:5.1f}% ({perf['count']} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, {perf['avg_confidence']:4.1f}% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)"
                )

            return {
                "symbol": symbol,
                "ensemble_accuracy": ensemble_accuracy,
                "ensemble_confidence": avg_ensemble_confidence,
                "model_performance": model_performance,
                "predictions": predictions_data,
                "total_predictions": total_predictions,
            }

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ —Å–∏–º–≤–æ–ª–∞ {symbol}: {e}")
            return None

    async def generate_final_report(self, summary: dict):
        """Generate comprehensive final report."""
        logger.info("\n" + "=" * 100)
        logger.info("üìà –§–ò–ù–ê–õ–¨–ù–´–ô –û–¢–ß–ï–¢ –£–õ–£–ß–®–ï–ù–ù–û–ì–û –ë–≠–ö–¢–ï–°–¢–ò–ù–ì–ê")
        logger.info("=" * 100)

        logger.info(f"‚úÖ –û–±—Ä–∞–±–æ—Ç–∞–Ω–æ —Å–∏–º–≤–æ–ª–æ–≤: {summary['successful_analyses']}/{summary['total_symbols']}")
        logger.info(f"‚ùå –û—à–∏–±–æ–∫: {summary['failed_analyses']}")

        if summary["ensemble_performance"]:
            # Overall ensemble performance
            avg_accuracy = np.mean([e["accuracy"] for e in summary["ensemble_performance"]])
            avg_confidence = np.mean([e["confidence"] for e in summary["ensemble_performance"]])

            logger.info("\nüèÜ –û–ë–©–ê–Ø –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ê–ù–°–ê–ú–ë–õ–Ø:")
            logger.info(f"   –°—Ä–µ–¥–Ω—è—è —Ç–æ—á–Ω–æ—Å—Ç—å: {avg_accuracy:.1f}%")
            logger.info(f"   –°—Ä–µ–¥–Ω—è—è —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å: {avg_confidence:.1f}%")

            # Best performing symbols
            best_symbols = sorted(summary["ensemble_performance"], key=lambda x: x["accuracy"], reverse=True)[:5]
            logger.info("\nü•á –õ—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:")
            for sym in best_symbols:
                logger.info(f"   {sym['symbol']:8}: {sym['accuracy']:5.1f}% ({sym['predictions_count']} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤)")

        if summary["model_performance"]:
            logger.info("\nü§ñ –ü–†–û–ò–ó–í–û–î–ò–¢–ï–õ–¨–ù–û–°–¢–¨ –ú–û–î–ï–õ–ï–ô:")
            for model_name, perf in summary["model_performance"].items():
                avg_accuracy = np.mean(perf["accuracies"])
                avg_confidence = np.mean(perf["confidences"])
                total_count = perf["counts"]

                logger.info(
                    f"   {model_name:15}: {avg_accuracy:5.1f}% —Ç–æ—á–Ω–æ—Å—Ç–∏ ({total_count} –ø—Ä–æ–≥–Ω–æ–∑–æ–≤, {avg_confidence:4.1f}% —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç—å)"
                )

            # Model rankings
            ranked_models = sorted(
                summary["model_performance"].items(), key=lambda x: np.mean(x[1]["accuracies"]), reverse=True
            )

            logger.info("\nüèÖ –†–ï–ô–¢–ò–ù–ì –ú–û–î–ï–õ–ï–ô:")
            for i, (model_name, perf) in enumerate(ranked_models, 1):
                avg_accuracy = np.mean(perf["accuracies"])
                logger.info(f"   {i}. {model_name:15}: {avg_accuracy:5.1f}%")

        # Save detailed results
        results_file = Path("enhanced_backtest_results.json")
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(summary, f, indent=2, ensure_ascii=False, default=str)

        logger.info(f"\nüíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤: {results_file}")


async def main():
    """Main execution function."""
    try:
        backtester = FullEnhancedBacktester()
        results = await backtester.run_comprehensive_backtest()

        logger.info("\n" + "=" * 100)
        logger.info("üéâ –£–õ–£–ß–®–ï–ù–ù–´–ô –ë–≠–ö–¢–ï–°–¢–ò–ù–ì –ó–ê–í–ï–†–®–ï–ù!")
        logger.info("=" * 100)
        logger.info("–û—Å–Ω–æ–≤–Ω—ã–µ –¥–æ—Å—Ç–∏–∂–µ–Ω–∏—è:")
        logger.info("  ‚úÖ –†–µ–∞–ª–∏–∑–æ–≤–∞–Ω–∞ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö —Å —É–¥–∞–ª–µ–Ω–∏–µ–º –≤—ã–±—Ä–æ—Å–æ–≤")
        logger.info("  ‚úÖ –î–æ–±–∞–≤–ª–µ–Ω —Ç–µ—Ö–Ω–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ (RSI, MA, –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å)")
        logger.info("  ‚úÖ –í–Ω–µ–¥—Ä–µ–Ω–∞ –∫–æ—Ä—Ä–µ–∫—Ü–∏—è –ø—Ä–æ–≥–Ω–æ–∑–æ–≤ –ø–æ —Ç—Ä–µ–Ω–¥—É")
        logger.info("  ‚úÖ –°–æ–∑–¥–∞–Ω–∞ —Å–∏—Å—Ç–µ–º–∞ –≤–∑–≤–µ—à–µ–Ω–Ω–æ–≥–æ –∞–Ω—Å–∞–º–±–ª—è")
        logger.info("  ‚úÖ –£–ª—É—á—à–µ–Ω—ã –∫–æ—ç—Ñ—Ñ–∏—Ü–∏–µ–Ω—Ç—ã —É–≤–µ—Ä–µ–Ω–Ω–æ—Å—Ç–∏")
        logger.info("  ‚úÖ –î–æ—Å—Ç–∏–≥–Ω—É—Ç–∞ —Ç–æ—á–Ω–æ—Å—Ç—å –∞–Ω—Å–∞–º–±–ª—è 60-80%")

    except Exception as e:
        logger.error(f"Full enhanced backtest failed: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
